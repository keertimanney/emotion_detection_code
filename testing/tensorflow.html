<!DOCTYPE html>
<html>
<head>
  <script src="./dist/face-api.js"></script>
  <script src="./js/faceDetectionControls.js"></script>
  <link rel="stylesheet" href="styles.css">
  <script src="https://cdn.jsdelivr.net/npm/opus-media-recorder@latest/OpusMediaRecorder.umd.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/opus-media-recorder@latest/encoderWorker.umd.js"></script>  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css">
  <script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js"></script>
</head>
<body>
  <div id="navbar"></div>
  <div class="center-content page-container">

    <div class="progress" id="loader">
      <div class="indeterminate"></div>
    </div>
    <div style="position: relative" class="margin">
      <video onloadedmetadata="onPlay(this)" id="inputVideo" autoplay muted playsinline></video>
      <audio onloadedmetadata="recordAudio(this)" id = "audioPlayer"></audio>
      <canvas id="overlay" />
    </div>

    <div class="row side-by-side">

      <!-- fps_meter -->
      <div id="fps_meter" class="row side-by-side">
        <div>
          <label for="time">Time:</label>
          <input disabled value="-" id="time" type="text" class="bold">
          <label for="fps">Estimated Fps:</label>
          <input disabled value="-" id="fps" type="text" class="bold">
        </div>
      </div>
      <!-- fps_meter -->

    </div>

    <input type="Emotion from voice will be displayed here" id="emotionOutput"> 


  </body>

  <script>
    let forwardTimes = []
    let withBoxes = true
    let max = 1000;
    let identifier = Math.floor(Math.random() * max);
    let audio_recording_start_time = Date.now();
    let last_data_send_time = Date.now();
    let recordedChunks = [];

    function onChangeHideBoundingBoxes(e) {
      withBoxes = !$(e.target).prop('checked')
    }

    function updateTimeStats(timeInMs) {
      forwardTimes = [timeInMs].concat(forwardTimes).slice(0, 30)
      const avgTimeInMs = forwardTimes.reduce((total, t) => total + t) / forwardTimes.length
      $('#time').val(`${Math.round(avgTimeInMs)} ms`)
      $('#fps').val(`${faceapi.utils.round(1000 / avgTimeInMs)}`)
    }

    async function recordAudio() {
        console.log("We are starting the audio recording");
        const audioEl = $("#audioPlayer").get(0);
        const options_audio = {mimeType: 'audio/wav'};
        window.MediaRecorder = OpusMediaRecorder;
        const mediaRecorder = new MediaRecorder(audioEl.srcObject, options_audio);


        mediaRecorder.addEventListener('dataavailable', function(e) {
            // console.log(e.data);
            // const reader = new FileReader();
            // reader.readAsDataURL(e.data);
            // reader.onloadend = function () {
            //     var base64String = reader.result;
            //     console.log('Base64 String - ', base64String);
            //     let time_now = Date.now();
            //     if (time_now - audio_recording_start_time > 7000.0) {
            //         console.log("Are we inside audio recording");
            //         $(() => {
            //             let output = $.post(
            //                 "https://192.168.1.205:4000/send_value_v3",
            //                 {
            //                     "data": base64String,
            //                     "identifier": identifier
            //                 },
            //                 function (output) {
            //                     console.log("whats the output from voice here", output);
            //                     document.getElementById("emotionOutput").value = output;
            //                 }
            //             );
            //         });
            //         // console.log("What is the output", output);
            //         audio_recording_start_time = time_now;
            //     }
            // }
            // console.log("What is the URL", response);
        });

        // How many milliseconds we want to record until
        mediaRecorder.start(2000);
    }

    async function onPlay() {
      const videoEl = $('#inputVideo').get(0);
      const audioEl = $('#audioPlayer').get(0);

      if(videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded())
        return setTimeout(() => onPlay())


      const options = getFaceDetectorOptions()

      const ts = Date.now()

      const result = await faceapi.detectSingleFace(videoEl, options).withFaceExpressions();
      const landmark_result = await faceapi.detectSingleFace(videoEl, options).withFaceLandmarks();


      updateTimeStats(Date.now() - ts)

      if (result) {
        const canvas = $('#overlay').get(0)
        const dims = faceapi.matchDimensions(canvas, videoEl, true)

        const resizedResult = faceapi.resizeResults(result, dims);
        let resizedResultLandmark = null;
        if (landmark_result !== undefined) {
            resizedResultLandmark = faceapi.resizeResults(landmark_result, dims)
        }
        const minConfidence = 0.05
        if (withBoxes) {
          const detection_data = faceapi.draw.drawDetections(canvas, resizedResult);
        //   console.log(detection_data);
          let expressions_angry = detection_data[0].expressions.angry;
          let expressions_neutral = detection_data[0].expressions.neutral;
          let expressions_sad = detection_data[0].expressions.sad;
          let expressions_happy = detection_data[0].expressions.happy;
          let expressions_surprised = detection_data[0].expressions.surprised;
          let expressions_disgusted = detection_data[0].expressions.disgusted;
          let expressions_fearful = detection_data[0].expressions.fearful;


          let time_now = Date.now();
          console.log(time_now, last_data_send_time);
          if (time_now - last_data_send_time > 100.0) {
              console.log("We are inside");
                $(() => {
                    let value = $.post(
                        'https://192.168.1.224:4000/send_value_v2', 
                        {
                            "identifier": identifier, 
                            "angry": expressions_angry,
                            "sad": expressions_sad,
                            "neutral": expressions_neutral,
                            "happy": expressions_happy,
                            "surprised": expressions_surprised,
                            "disgusted": expressions_disgusted,
                            "fearful": expressions_fearful,
                        }
                    );
                    console.log(value);
                });
                last_data_send_time = time_now;
                console.log(detection_data[0].expressions);
            }
        }
        faceapi.draw.drawFaceExpressions(canvas, resizedResult, minConfidence);
        if (resizedResultLandmark !== null) {
            faceapi.draw.drawFaceLandmarks(canvas, resizedResultLandmark);
        }
      }

      setTimeout(() => onPlay())
    }

    async function run() {
      // load face detection and face expression recognition models
      await changeFaceDetector(TINY_FACE_DETECTOR)
      await faceapi.loadFaceExpressionModel('/');
      await faceapi.loadFaceLandmarkModel('/');
      changeInputSize(224)

      // try to access users webcam and stream the images
      // to the video element
      const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true })
      const videoEl = $('#inputVideo').get(0);
      const audioEl = $('#audioPlayer').get(0);
      audioEl.srcObject = stream;
      videoEl.srcObject = stream;
    }

    function updateResults() {}

    $(document).ready(function() {
      // renderNavBar('#navbar', 'webcam_face_expression_recognition')
      initFaceDetectionControls()
      run()
    })
  </script>
</body>
</html>